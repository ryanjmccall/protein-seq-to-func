{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3958a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from scripts.build_index import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe2f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load data from multiple sources\n",
    "all_documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f446764a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 307 GenAge documents\n"
     ]
    }
   ],
   "source": [
    "genage_docs = load_genage_data(\"../data/raw/genage_human.csv\")\n",
    "all_documents.extend(genage_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ccff82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 8 UniProt documents\n"
     ]
    }
   ],
   "source": [
    "uniprot_docs = load_uniprot_data(\"../data/raw/uniprot_sequences.csv\")\n",
    "all_documents.extend(uniprot_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05c77b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Searching Europe PMC for CCR1...\n",
      "[EPMC SEARCH] Searching for: CCR1\n",
      "✅ Found 3 Europe PMC articles for CCR1\n",
      "  📄 Getting full text for highly cited article: PMC2212798\n",
      "  📄 Getting full text for highly cited article: PMC4993001\n",
      "  📄 Getting full text for highly cited article: PMC8273155\n",
      "\n",
      "🔍 Searching Europe PMC for CCR2...\n",
      "[EPMC SEARCH] Searching for: CCR2\n",
      "✅ Found 3 Europe PMC articles for CCR2\n",
      "  📄 Getting full text for highly cited article: PMC6340744\n",
      "  📄 Getting full text for highly cited article: PMC7402632\n",
      "  📄 Getting full text for highly cited article: PMC7264227\n",
      "\n",
      "🔍 Searching Europe PMC for CCR5...\n",
      "[EPMC SEARCH] Searching for: CCR5\n",
      "✅ Found 3 Europe PMC articles for CCR5\n",
      "  📄 Getting full text for highly cited article: PMC7095016\n",
      "  📄 Getting full text for highly cited article: PMC7167720\n",
      "  📄 Getting full text for highly cited article: PMC4433489\n",
      "\n",
      "🔍 Searching Europe PMC for CCR7...\n",
      "[EPMC SEARCH] Searching for: CCR7\n",
      "✅ Found 3 Europe PMC articles for CCR7\n",
      "  📄 Getting full text for highly cited article: PMC8238499\n",
      "  📄 Getting full text for highly cited article: PMC6884693\n",
      "  📄 Getting full text for highly cited article: PMC7889871\n",
      "\n",
      "🔍 Searching Europe PMC for NRF2...\n",
      "[EPMC SEARCH] Searching for: NRF2\n",
      "✅ Found 3 Europe PMC articles for NRF2\n",
      "  📄 Getting full text for highly cited article: PMC5864239\n",
      "  📄 Getting full text for highly cited article: PMC2799634\n",
      "  📄 Getting full text for highly cited article: PMC4066722\n",
      "\n",
      "🔍 Searching Europe PMC for APOE...\n",
      "[EPMC SEARCH] Searching for: APOE\n",
      "✅ Found 3 Europe PMC articles for APOE\n",
      "  📄 Getting full text for highly cited article: PMC4544753\n",
      "  📄 Getting full text for highly cited article: PMC5958625\n",
      "  📄 Getting full text for highly cited article: PMC7392084\n",
      "\n",
      "🔍 Searching Europe PMC for SOX2...\n",
      "[EPMC SEARCH] Searching for: SOX2\n",
      "✅ Found 3 Europe PMC articles for SOX2\n",
      "  📄 Getting full text for highly cited article: PMC3637064\n",
      "  📄 Getting full text for highly cited article: PMC7889871\n",
      "  📄 Getting full text for highly cited article: PMC3431493\n",
      "\n",
      "🔍 Searching Europe PMC for OCT4...\n",
      "[EPMC SEARCH] Searching for: OCT4\n",
      "✅ Found 3 Europe PMC articles for OCT4\n",
      "  📄 Getting full text for highly cited article: PMC8454663\n",
      "  📄 Getting full text for highly cited article: PMC4530010\n",
      "  📄 Getting full text for highly cited article: PMC4433489\n",
      "✅ Loaded 48 Europe PMC documents\n"
     ]
    }
   ],
   "source": [
    "key_proteins = [\"CCR1\", \"CCR2\", \"CCR5\", \"CCR7\", \"NRF2\", \"APOE\", \"SOX2\", \"OCT4\"]  # Example proteins\n",
    "europepmc_docs = load_europepmc_data(key_proteins, max_articles_per_protein=3)\n",
    "all_documents.extend(europepmc_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e5834c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔨 Building in-memory index...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtarc\\Dev\\protein-seq-to-func\\.conda\\Lib\\site-packages\\llama_index\\core\\embeddings\\utils.py:59\u001b[39m, in \u001b[36mresolve_embed_model\u001b[39m\u001b[34m(embed_model, callback_manager)\u001b[39m\n\u001b[32m     58\u001b[39m     embed_model = OpenAIEmbedding()\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[43mvalidate_openai_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtarc\\Dev\\protein-seq-to-func\\.conda\\Lib\\site-packages\\llama_index\\embeddings\\openai\\utils.py:104\u001b[39m, in \u001b[36mvalidate_openai_api_key\u001b[39m\u001b[34m(api_key)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[31mValueError\u001b[39m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m in_memory_index = \u001b[43mbuild_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_chromadb\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m in_memory_query_engine = in_memory_index.as_query_engine()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtarc\\Dev\\protein-seq-to-func\\notebooks\\..\\scripts\\build_index.py:268\u001b[39m, in \u001b[36mbuild_index\u001b[39m\u001b[34m(documents, use_chromadb)\u001b[39m\n\u001b[32m    266\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Index built and stored in ChromaDB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔨 Building in-memory index...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    269\u001b[39m     index = VectorStoreIndex.from_documents(documents)\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ In-memory index built\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtarc\\Dev\\protein-seq-to-func\\.conda\\Lib\\site-packages\\llama_index\\core\\indices\\base.py:122\u001b[39m, in \u001b[36mBaseIndex.from_documents\u001b[39m\u001b[34m(cls, documents, storage_context, show_progress, callback_manager, transformations, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m     docstore.set_document_hash(doc.id_, doc.hash)\n\u001b[32m    115\u001b[39m nodes = run_transformations(\n\u001b[32m    116\u001b[39m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    117\u001b[39m     transformations,\n\u001b[32m    118\u001b[39m     show_progress=show_progress,\n\u001b[32m    119\u001b[39m     **kwargs,\n\u001b[32m    120\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtarc\\Dev\\protein-seq-to-func\\.conda\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:71\u001b[39m, in \u001b[36mVectorStoreIndex.__init__\u001b[39m\u001b[34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28mself\u001b[39m._use_async = use_async\n\u001b[32m     69\u001b[39m \u001b[38;5;28mself\u001b[39m._store_nodes_override = store_nodes_override\n\u001b[32m     70\u001b[39m \u001b[38;5;28mself\u001b[39m._embed_model = resolve_embed_model(\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     embed_model \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSettings\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_model\u001b[49m, callback_manager=callback_manager\n\u001b[32m     72\u001b[39m )\n\u001b[32m     74\u001b[39m \u001b[38;5;28mself\u001b[39m._insert_batch_size = insert_batch_size\n\u001b[32m     75\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     76\u001b[39m     nodes=nodes,\n\u001b[32m     77\u001b[39m     index_struct=index_struct,\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m     **kwargs,\n\u001b[32m     84\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtarc\\Dev\\protein-seq-to-func\\.conda\\Lib\\site-packages\\llama_index\\core\\settings.py:64\u001b[39m, in \u001b[36m_Settings.embed_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the embedding model.\"\"\"\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embed_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28mself\u001b[39m._embed_model = \u001b[43mresolve_embed_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdefault\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mself\u001b[39m._embed_model.callback_manager = \u001b[38;5;28mself\u001b[39m._callback_manager\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtarc\\Dev\\protein-seq-to-func\\.conda\\Lib\\site-packages\\llama_index\\core\\embeddings\\utils.py:66\u001b[39m, in \u001b[36mresolve_embed_model\u001b[39m\u001b[34m(embed_model, callback_manager)\u001b[39m\n\u001b[32m     61\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     62\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`llama-index-embeddings-openai` package not found, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     63\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mplease run `pip install llama-index-embeddings-openai`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     64\u001b[39m         )\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     67\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCould not load OpenAI embedding model. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mConsider using embed_model=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mlocal\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mVisit our documentation for more embedding options: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttps://docs.llamaindex.ai/en/stable/module_guides/models/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33membeddings.html#modules\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m******\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m         )\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# for image multi-modal embeddings\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m embed_model.startswith(\u001b[33m\"\u001b[39m\u001b[33mclip\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: \n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******"
     ]
    }
   ],
   "source": [
    "in_memory_index = build_index(all_documents, use_chromadb=False)\n",
    "in_memory_query_engine = in_memory_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f294dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"What are the full names of the genes with symbols NRF2, SOX2, APOE, OCT4?\",\n",
    "    \"What proteins are related to longevity and aging?\",\n",
    "    \"What are the most cited research findings about CCR5?\",\n",
    "    \"What are the functions of CCR5 protein?\",\n",
    "    \"What research has been done on transcription factors like SOX2 and OCT4?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce4a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query_engine(in_memory_query_engine, test_questions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
